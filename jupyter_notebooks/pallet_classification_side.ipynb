{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xgY6ky0OmvEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip classification_dataset.zip -d dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RArpGBevhccR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Разделение данных и балансировка"
      ],
      "metadata": {
        "id": "Ztaip9CzhQ2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir = '/content/dataset/classification_dataset'\n",
        "destination_dir_bottom = 'pallet_bottom_dataset'\n",
        "destination_dir_side = 'pallet_side_dataset'\n",
        "\n",
        "os.makedirs(destination_dir_bottom, exist_ok=True)\n",
        "os.makedirs(destination_dir_side, exist_ok=True)\n",
        "\n",
        "def copy_class_files(source, dest):\n",
        "    for file_name in os.listdir(source):\n",
        "        src_file = os.path.join(source, file_name)\n",
        "        dest_file = os.path.join(dest, file_name)\n",
        "        shutil.copy2(src_file, dest_file)\n",
        "\n",
        "bottom_classes = ['good_pallet', 'replace_pallet']\n",
        "for class_name in bottom_classes:\n",
        "    source_class_path = os.path.join(source_dir, 'pallet_bottom', class_name)\n",
        "    dest_class_path = os.path.join(destination_dir_bottom, class_name)\n",
        "    os.makedirs(dest_class_path, exist_ok=True)\n",
        "    copy_class_files(source_class_path, dest_class_path)\n",
        "\n",
        "\n",
        "side_classes = ['good_pallet', 'replace_pallet']\n",
        "for class_name in side_classes:\n",
        "    source_class_path = os.path.join(source_dir, 'pallet_side', class_name)\n",
        "    dest_class_path = os.path.join(destination_dir_side, class_name)\n",
        "    os.makedirs(dest_class_path, exist_ok=True)\n",
        "    copy_class_files(source_class_path, dest_class_path)\n",
        "\n",
        "print(\"Разделение датасета завершено.\")\n"
      ],
      "metadata": {
        "id": "ffhLHbDDhJtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
        "\n",
        "dataset_dir = '/content/pallet_side_dataset'\n",
        "class_folders = ['good_pallet', 'replace_pallet']\n",
        "\n",
        "class_counts = {cls: len(os.listdir(os.path.join(dataset_dir, cls))) for cls in class_folders}\n",
        "\n",
        "minority_class = min(class_counts, key=class_counts.get)\n",
        "majority_class = max(class_counts, key=class_counts.get)\n",
        "\n",
        "minority_count = class_counts[minority_class]\n",
        "majority_count = class_counts[majority_class]\n",
        "\n",
        "print(f\"Миноритарная папка: {minority_class}, количество изображений: {minority_count}\")\n",
        "print(f\"Мажоритарная папка: {majority_class}, количество изображений: {majority_count}\")\n",
        "\n",
        "num_to_add = majority_count - minority_count\n",
        "\n",
        "minority_folder = os.path.join(dataset_dir, minority_class)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "images = os.listdir(minority_folder)\n",
        "i = 0\n",
        "while i < num_to_add:\n",
        "    img_name = images[i % len(images)]\n",
        "    img_path = os.path.join(minority_folder, img_name)\n",
        "\n",
        "    img = load_img(img_path)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array.reshape((1,) + img_array.shape)\n",
        "\n",
        "    for batch in datagen.flow(img_array, batch_size=1, save_to_dir=minority_folder, save_prefix='aug', save_format='jpeg'):\n",
        "        i += 1\n",
        "        if i >= num_to_add:\n",
        "            break\n",
        "\n",
        "print(f\"Теперь в папке '{minority_class}' {minority_count + num_to_add} изображений, как и в '{majority_class}'.\")\n"
      ],
      "metadata": {
        "id": "6KYkzFCPhMf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение кастомной модели"
      ],
      "metadata": {
        "id": "relGlnWyhVRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data_dir = '/content/pallet_side_dataset'\n",
        "\n",
        "\n",
        "img_size = (224, 224)\n",
        "\n",
        "def create_data_generators(data_dir, img_size, batch_size):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1.0 / 255,\n",
        "        validation_split=0.3,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=30,\n",
        "        brightness_range=[0.7, 1.3]\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    val_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "def build_mobilenetv2_model(input_shape, dropout_rate):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-1, log=True)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.8)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [2, 4, 8, 16, 32])\n",
        "    epochs = trial.suggest_int('epochs', 10, 60)\n",
        "\n",
        "    train_gen, val_gen = create_data_generators(data_dir, img_size, batch_size)\n",
        "\n",
        "    model = build_mobilenetv2_model((img_size[0], img_size[1], 3), dropout_rate)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, mode='min')\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_loss = min(history.history['val_loss'])\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "train_gen, val_gen = create_data_generators(data_dir, img_size, best_params['batch_size'])\n",
        "model = build_mobilenetv2_model((img_size[0], img_size[1], 3), best_params['dropout_rate'])\n",
        "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=best_params['epochs'],\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, mode='min')]\n",
        ")\n",
        "\n",
        "val_gen.reset()\n",
        "val_pred_prob = model.predict(val_gen)\n",
        "val_pred = (val_pred_prob > 0.5).astype(int)\n",
        "val_true = val_gen.classes\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(val_true, val_pred, target_names=['good_pallet', 'replace_pallet']))\n"
      ],
      "metadata": {
        "id": "L8oThDfTyEzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.saving.save_model(model, 'pallet_side_classifier_mn_optuna.h5')"
      ],
      "metadata": {
        "id": "UxjoEjfEjunA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестирование модели"
      ],
      "metadata": {
        "id": "n96VGUb_YfVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, **kwargs):\n",
        "        super(FocalLoss, self).__init__(**kwargs)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, dtype='float32')\n",
        "        alpha_t = y_true * self.alpha + (tf.ones_like(y_true) - y_true) * (1 - self.alpha)\n",
        "        p_t = y_true * y_pred + (tf.ones_like(y_true) - y_true) * (tf.ones_like(y_true) - y_pred) + tf.keras.backend.epsilon()\n",
        "        focal_loss = -alpha_t * tf.pow((tf.ones_like(y_true) - p_t), self.gamma) * tf.math.log(p_t)\n",
        "        return tf.reduce_mean(focal_loss)\n",
        "\n",
        "model = tf.keras.models.load_model(\n",
        "    '/content/pallet_side_classifier_mn_optuna.h5',\n",
        "    custom_objects={'FocalLoss': FocalLoss}\n",
        ")\n",
        "\n",
        "def prepare_image(image_path, target_size=(224, 224)):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "image_path = '/content/0_IMG_2863.jpg'\n",
        "prepared_image = prepare_image(image_path)\n",
        "predictions = model.predict(prepared_image)\n",
        "\n",
        "print(f\"Вероятность принадлежности к классу 'replace_pallet': {predictions[0][0]}\")\n",
        "\n",
        "predicted_class = (predictions > 0.5).astype(int)[0][0]\n",
        "class_labels = ['good_pallet', 'replace_pallet']\n",
        "\n",
        "print(f\"Предсказанный класс: {class_labels[predicted_class]}\")\n"
      ],
      "metadata": {
        "id": "GsuYI6sQrWXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}