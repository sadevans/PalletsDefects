{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9349751,
          "sourceType": "datasetVersion",
          "datasetId": 5667390
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Разделение данных"
      ],
      "metadata": {
        "id": "qLOmjPd6vuIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir = '/kaggle/input/pallet-classification/classification_dataset'\n",
        "destination_dir_bottom = '/kaggle/working/pallet_bottom_dataset'\n",
        "destination_dir_side = '/kaggle/working/pallet_side_dataset'\n",
        "\n",
        "os.makedirs(destination_dir_bottom, exist_ok=True)\n",
        "os.makedirs(destination_dir_side, exist_ok=True)\n",
        "\n",
        "# Функция для копирования файлов из исходной папки в целевую\n",
        "def copy_class_files(source, dest):\n",
        "    for file_name in os.listdir(source):\n",
        "        src_file = os.path.join(source, file_name)\n",
        "        dest_file = os.path.join(dest, file_name)\n",
        "        shutil.copy2(src_file, dest_file)\n",
        "\n",
        "# Копируем файлы для pallet_bottom\n",
        "bottom_classes = ['good_pallet', 'replace_pallet']\n",
        "for class_name in bottom_classes:\n",
        "    source_class_path = os.path.join(source_dir, 'pallet_bottom', class_name)\n",
        "    dest_class_path = os.path.join(destination_dir_bottom, class_name)\n",
        "    os.makedirs(dest_class_path, exist_ok=True)\n",
        "    copy_class_files(source_class_path, dest_class_path)\n",
        "\n",
        "# Копируем файлы для pallet_side\n",
        "side_classes = ['good_pallet', 'replace_pallet']\n",
        "for class_name in side_classes:\n",
        "    source_class_path = os.path.join(source_dir, 'pallet_side', class_name)\n",
        "    dest_class_path = os.path.join(destination_dir_side, class_name)\n",
        "    os.makedirs(dest_class_path, exist_ok=True)\n",
        "    copy_class_files(source_class_path, dest_class_path)\n",
        "\n",
        "print(\"Разделение датасета завершено.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T17:57:47.143850Z",
          "iopub.execute_input": "2024-09-11T17:57:47.144814Z",
          "iopub.status.idle": "2024-09-11T17:57:47.945472Z",
          "shell.execute_reply.started": "2024-09-11T17:57:47.144770Z",
          "shell.execute_reply": "2024-09-11T17:57:47.944249Z"
        },
        "trusted": true,
        "id": "uDCy0UIdvuIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Балансировка классов"
      ],
      "metadata": {
        "id": "ULMyEpU-vuIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "dataset_dir = '/kaggle/working/pallet_bottom_dataset'\n",
        "class_folders = ['good_pallet', 'replace_pallet']\n",
        "\n",
        "target_count = 50\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "for class_folder in class_folders:\n",
        "    folder_path = os.path.join(dataset_dir, class_folder)\n",
        "    images = os.listdir(folder_path)\n",
        "    current_count = len(images)\n",
        "    num_to_add = target_count - current_count\n",
        "\n",
        "    print(f\"Папка '{class_folder}', текущее количество: {current_count}, нужно добавить: {num_to_add}\")\n",
        "\n",
        "    i = 0\n",
        "    while i < num_to_add:\n",
        "        img_name = images[i % len(images)]\n",
        "        img_path = os.path.join(folder_path, img_name)\n",
        "\n",
        "        img = load_img(img_path)\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = img_array.reshape((1,) + img_array.shape)\n",
        "\n",
        "        for batch in datagen.flow(img_array, batch_size=1, save_to_dir=folder_path, save_prefix='aug', save_format='jpeg'):\n",
        "            i += 1\n",
        "            if i >= num_to_add:\n",
        "                break\n",
        "\n",
        "print(\"Аугментация завершена. Теперь в каждой папке по 50 изображений.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T17:57:51.860227Z",
          "iopub.execute_input": "2024-09-11T17:57:51.860638Z",
          "iopub.status.idle": "2024-09-11T17:57:53.110562Z",
          "shell.execute_reply.started": "2024-09-11T17:57:51.860594Z",
          "shell.execute_reply": "2024-09-11T17:57:53.109403Z"
        },
        "trusted": true,
        "id": "irrrvj8QvuIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import random\n",
        "\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"Using GPU: {gpus[0]}\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"GPU not available, using CPU.\")\n",
        "\n",
        "data_dir = '/kaggle/working/pallet_bottom_dataset'\n",
        "\n",
        "img_size = (224, 224)\n",
        "batch_size = 2\n",
        "epochs = 50\n",
        "\n",
        "def create_data_generators(data_dir, img_size, batch_size):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1.0 / 255,\n",
        "        validation_split=0.2,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.3,\n",
        "        rotation_range=30,\n",
        "        brightness_range=[0.3, 1.3]\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    val_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "train_gen, val_gen = create_data_generators(data_dir, img_size, batch_size)\n",
        "\n",
        "def build_efficientnetb3_model(input_shape, dropout_rate=0.5, l2_lambda=0.0002):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=inputs)\n",
        "\n",
        "    # Определение количества слоев для разморозки\n",
        "    num_layers = len(base_model.layers)\n",
        "    num_unfreeze = int(num_layers * 0.25)\n",
        "\n",
        "    # Заморозка нижних 75% слоев\n",
        "    for layer in base_model.layers[:num_layers - num_unfreeze]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Разморозка верхних 25% слоев\n",
        "    for layer in base_model.layers[num_layers - num_unfreeze:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Добавление дополнительных слоев\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_lambda))(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = build_efficientnetb3_model(input_shape = (img_size[0], img_size[1], 3))\n",
        "model.compile(\n",
        "    optimizer=AdamW(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "class MetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_gen.reset()\n",
        "        val_pred_prob = self.model.predict(val_gen)\n",
        "        val_pred = (val_pred_prob > 0.5).astype(int)\n",
        "        val_true = val_gen.classes\n",
        "\n",
        "        precision = precision_score(val_true, val_pred, average='binary')\n",
        "        recall = recall_score(val_true, val_pred, average='binary')\n",
        "        f1 = f1_score(val_true, val_pred, average='binary')\n",
        "        cm = confusion_matrix(val_true, val_pred)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1} - Additional Metrics:\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "        print(f\"False Negatives (FN): {cm[1, 0]}, False Positives (FP): {cm[0, 1]}\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_gen,\n",
        "    callbacks=[MetricsCallback()]\n",
        ")\n",
        "\n",
        "val_gen.reset()\n",
        "val_pred_prob = model.predict(val_gen)\n",
        "val_pred = (val_pred_prob > 0.5).astype(int)\n",
        "val_true = val_gen.classes\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(val_true, val_pred, target_names=['good_pallet', 'replace_pallet']))\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-11T18:08:34.147899Z",
          "iopub.execute_input": "2024-09-11T18:08:34.148802Z",
          "iopub.status.idle": "2024-09-11T18:11:35.806569Z",
          "shell.execute_reply.started": "2024-09-11T18:08:34.148749Z",
          "shell.execute_reply": "2024-09-11T18:11:35.805025Z"
        },
        "trusted": true,
        "id": "GHuy0YjkvuIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.saving.save_model(model, 'pallet_bottom_classifier_mn_optuna.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T18:04:46.574384Z",
          "iopub.execute_input": "2024-09-11T18:04:46.575536Z",
          "iopub.status.idle": "2024-09-11T18:04:47.035347Z",
          "shell.execute_reply.started": "2024-09-11T18:04:46.575477Z",
          "shell.execute_reply": "2024-09-11T18:04:47.034140Z"
        },
        "trusted": true,
        "id": "32vQjcyivuI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}